<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Qingqing Ge&#39;s Blogs</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-04-01T02:31:13.224Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Crystal Ge 葛青青</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>AS-UNet</title>
    <link href="http://example.com/2022/05/23/as-unet/"/>
    <id>http://example.com/2022/05/23/as-unet/</id>
    <published>2022-05-23T03:43:33.000Z</published>
    <updated>2023-04-01T02:31:13.224Z</updated>
    
    <content type="html"><![CDATA[<p>《一种具有边缘增强特点的医学图像分割网络》<br>发表于《电子与信息学报》</p><p><a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=16490eb09f3j0vk05b6f0ew0f6183856&site=xueshu_se&hitarticle=1">pdf</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>针对传统医学图像分割网络存在边缘分割不清晰、缺失值大等问题，该文提出一种具有边缘增强特点的医学图像分割网络(AS-UNet)。利用掩膜边缘提取算法得到掩膜边缘图，在UNet扩张路径的最后3层引入结合多尺度特征图的边缘注意模块(BAB)，并提出组合损失函数来提高分割精度；测试时通过舍弃BAB来减少参数。在3种不同类型的医学图像分割数据集Glas,DRIVE,ISIC2018上进行实验，与其它分割方法相比，AS-UNet分割性能较优。</p><h2 id="模型结构图"><a href="#模型结构图" class="headerlink" title="模型结构图"></a>模型结构图</h2><p><img src="/./img/AS-UNet.png" alt="Image discription"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;《一种具有边缘增强特点的医学图像分割网络》&lt;br&gt;发表于《电子与信息学报》&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://xueshu.baidu.com/usercenter/paper/show?paperid=16490eb09f3j0vk05b6f0ew0f618</summary>
      
    
    
    
    
    <category term="CV" scheme="http://example.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Alzheimer’s Disease Classfication</title>
    <link href="http://example.com/2021/07/03/alzheimer/"/>
    <id>http://example.com/2021/07/03/alzheimer/</id>
    <published>2021-07-03T03:43:33.000Z</published>
    <updated>2023-04-01T02:36:52.262Z</updated>
    
    <content type="html"><![CDATA[<p>《A Lightweight Spatial Attention Module with Adaptive Receptive Fields in 3D Convolutional Neural Network for Alzheimer’s Disease Classfication》<br>发表于《Springer Nature Switzerland AG》</p><p><a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=1k4q0jv0s0750ex01q5d0ra0bk480396">pdf</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>The development of deep learning provides powerful support for disease classification of neuroimaging data. However, in the classification of neuroimaging data based on deep learning methods, the spatial information cannot be fully utilized. In this paper, we propose a lightweight 3D spatial attention module with adaptive receptive fields, which allows neurons to adaptively adjust the receptive field size according to multiple scales of input information. The attention module can fuse spatial information of different scales on multiple branches, so that 3D spatial information of neuroimaging data can be fully utilized. A 3D-ResNet18 based on our proposed attention module is trained to diagnose Alzheimer’s disease (AD). Experiments are conducted on 521 subjects (254 of patients with AD and 267 of normal controls) from Alzheimer’s Disease National Initiative (ADNI) dataset of 3D structural MRI brain scans. Experimental results show the effectiveness and efficiency of our proposed approach for AD classification.</p><h2 id="模型结构图"><a href="#模型结构图" class="headerlink" title="模型结构图"></a>模型结构图</h2><p><img src="/./img/al.png" alt="Image discription"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;《A Lightweight Spatial Attention Module with Adaptive Receptive Fields in 3D Convolutional Neural Network for Alzheimer’s Disease Classfi</summary>
      
    
    
    
    
    <category term="CV" scheme="http://example.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Alzheimer’s Disease Classfication</title>
    <link href="http://example.com/2021/07/03/blog/"/>
    <id>http://example.com/2021/07/03/blog/</id>
    <published>2021-07-03T03:43:33.000Z</published>
    <updated>2023-04-01T02:36:52.262Z</updated>
    
    <content type="html"><![CDATA[<p>《A Lightweight Spatial Attention Module with Adaptive Receptive Fields in 3D Convolutional Neural Network for Alzheimer’s Disease Classfication》<br>发表于《Springer Nature Switzerland AG》</p><p><a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=1k4q0jv0s0750ex01q5d0ra0bk480396">pdf</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>The development of deep learning provides powerful support for disease classification of neuroimaging data. However, in the classification of neuroimaging data based on deep learning methods, the spatial information cannot be fully utilized. In this paper, we propose a lightweight 3D spatial attention module with adaptive receptive fields, which allows neurons to adaptively adjust the receptive field size according to multiple scales of input information. The attention module can fuse spatial information of different scales on multiple branches, so that 3D spatial information of neuroimaging data can be fully utilized. A 3D-ResNet18 based on our proposed attention module is trained to diagnose Alzheimer’s disease (AD). Experiments are conducted on 521 subjects (254 of patients with AD and 267 of normal controls) from Alzheimer’s Disease National Initiative (ADNI) dataset of 3D structural MRI brain scans. Experimental results show the effectiveness and efficiency of our proposed approach for AD classification.</p><h2 id="模型结构图"><a href="#模型结构图" class="headerlink" title="模型结构图"></a>模型结构图</h2><p><img src="/./img/al.png" alt="Image discription"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;《A Lightweight Spatial Attention Module with Adaptive Receptive Fields in 3D Convolutional Neural Network for Alzheimer’s Disease Classfi</summary>
      
    
    
    
    
    <category term="CV" scheme="http://example.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv4</title>
    <link href="http://example.com/2021/02/28/yolov4/"/>
    <id>http://example.com/2021/02/28/yolov4/</id>
    <published>2021-02-28T03:43:33.000Z</published>
    <updated>2023-04-01T02:25:10.947Z</updated>
    
    <content type="html"><![CDATA[<p>《融合环境特征与改进YOLOv4的安全帽佩戴检测方法》<br>发表于《中国图象图形学报》<br>荣获该学报2022年度优秀论文</p><p><a href="http://www.cjig.cn/html/jig/2021/12/20211213.htm">pdf</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>目的 在施工现场，安全帽是最为常见和实用的个人防护用具，能够有效防止和减轻意外带来的头部伤害。但在施工现场的安全帽佩戴检测任务中，经常出现难以检测到小目标，或因为复杂多变的环境因素导致检测准确率降低等情况。针对这些问题，提出了一种融合环境特征与改进YOLOv4的安全帽佩戴检测方法以实现高效检测。方法 为补充丢失的特征，在保证感受野一致的情况下，将YOLOv4得到的三种不同大小的输出特征图与原图经过特征提取得到的特征图相加，从而将高层特征与低层特征融合，捕捉更多细节信息；对融合后的特征图采用3 x 3的卷积操作，以减小特征图融合后的混叠效应，保证特征稳定性，使得模型在大小目标检测上均能达到更高的准确率；同时，为适应施工现场的各种环境，采用多种数据增强方式进行环境模拟，并采用对抗训练方法增强模型的泛化能力和鲁棒性。结果 本文提出的改进YOLOv4方法在开源安全帽佩戴检测数据集（Safety Helmet Wearing Dataset, SHWD）上进行测试，mAP（Mean Average Precision）达到91.55%，recall达到98.62%。将该方法与CornerNet-Lite，Faster RCNN，YOLOv3，YOLOv4等安全帽佩戴检测算法进行对比，结果显示本文方法在mAP和recall上均有较大提升，其中相较于现有的YOLOv4算法，mAP和recall分别提高了5.2%和5.79%。并且，融合环境特征进行数据增强后，上述五种方法的mAP均有提高，提高2%～5%不等，其中本文提出的改进YOLOv4方法提高了4.27%，达到95.82%。此外，相较于数据增强前，改进YOLOv4在数据增强后的各种真实环境条件下进行测试时，都有较稳定的表现，检测准确率依然能保持在较高水平，其中对夜晚图片的检测效果提升尤其明显，mAP从67.73%提升至84.10%。结论 本文所提出的融合环境特征与改进YOLOv4的安全帽佩戴检测方法，以改进模型和数据增强的方式提升模型准确率、泛化能力和鲁棒性，为安全帽佩戴检测提供了有效保障。</p><h2 id="模型结构图"><a href="#模型结构图" class="headerlink" title="模型结构图"></a>模型结构图</h2><p><img src="/./img/yolov4.png" alt="Image discription"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;《融合环境特征与改进YOLOv4的安全帽佩戴检测方法》&lt;br&gt;发表于《中国图象图形学报》&lt;br&gt;荣获该学报2022年度优秀论文&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.cjig.cn/html/jig/2021/12/20211213.htm&quot;&gt;pdf&lt;/a&gt;</summary>
      
    
    
    
    
    <category term="CV" scheme="http://example.com/tags/CV/"/>
    
  </entry>
  
</feed>
